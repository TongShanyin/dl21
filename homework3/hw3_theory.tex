\documentclass[10pt,a4paper]{article}
\usepackage[centertags]{amsmath}
\usepackage{amsfonts,amssymb, amsthm}
\usepackage{hyperref}
\usepackage{comment}
\usepackage[shortlabels]{enumitem}
\usepackage{bm}

\usepackage{cite,graphicx,color}
%\usepackage{fourier}
\usepackage[margin=1.5in]{geometry}
\usepackage{enumitem}
\usepackage{bbm}

\usepackage{tikz,pgfplots}

\usepackage{mathtools}
%\mathtoolsset{showonlyrefs} % only show no. of refered eqs

\usepackage{cleveref}

\textheight 8.5in

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}

\newtheoremstyle{dotlessP}{}{}{\color{blue!50!black}}{}{\color{blue}\bfseries}{}{ }{}
\theoremstyle{dotlessP}
\newtheorem{question}{Question}



\def\VV{\mathbb{V}}
\def\EE{\mathbb{E}}
\def\PP{\mathbb{P}}
\def\RR{\mathbb{R}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mF}{F}%{\mathcal{F}}

\DeclareMathOperator{\sgn}{sgn}
%\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\erfc}{erfc}
\DeclareRobustCommand{\argmin}{\operatorname*{argmin}}
\DeclareRobustCommand{\arginf}{\operatorname*{arginf}}

\def\EE{\mathbb{E}}\def\PP{\mathbb{P}}
\def\NN{\mathbb{N}}\def\RR{\mathbb{R}}\def\ZZ{\mathbb{Z}}



\def\<{\left\langle} \def\>{\right\rangle}


%\DeclareRobustCommand{\linear}{\operatorname*{Linear}}
%\DeclareRobustCommand{\loss}{\operatorname*{Loss}}
%\DeclareRobustCommand{\diag}{\operatorname*{diag}}

\newcommand{\linear}{\text{Linear}}
\newcommand{\loss}{\text{Loss}}
\newcommand{\diag}{\text{diag}}




\newcommand{\emphasis}[1]{\textcolor{red!80!black}{#1}}
\newcommand{\shanyin}[1]{\textcolor{blue!80!black}{#1}}

% ****************************
\begin{document}


\title{Deep learning HW3}
\author{Shanyin Tong, st3255@nyu.edu}

\maketitle

\section{Theory}

\subsection{Energy Based Models Intuition}
\begin{enumerate}[(a)]
	\item Energy based model build/train energy function that has lower value at the good points $y_i$'s but has higher value at the bad points, i.e., other $\hat{y}_i$'s. In a word, for the energy function, $F_W$, $F_W(x_i, y_i)$ is small, while $F_W(x_i, \hat{y}_i)$ is large, i.e., push down on the energy of training samples and pull up on the energy for the other points.
	\item Probabilistic models are a special case of EBM. Energies are like unnormalized negative log probabilities. However, EBM gives more flexibility in the choice of the scoring function, and more flexibility in the choice of objective function for learning.
	\item \begin{equation}
	p(y|x) =\frac{\exp(-\beta F_W(x,y))}{\int_{y'}\exp(-\beta F_W(x,y'))}, \; \beta >0.
	\end{equation}
	\item Energy function is an implicit function that captures the dependency between training data, i.e., $x$ and $y$. Loss function is used for training/learning the energy function, which we want to minimize. When minimizing the loss, EBM pushes down the energy of the data points and pulls up everywhere else.
	\item Yes, loss function can equal to the energy function.
	\item Because the model don't know where to pull up the energy because only positive data are given. So to pull down the energy for the correct inputs, one naive way the model might do is to reduce energy for the whole field, i.e., a degenerate solution.
	\item \begin{enumerate}[(1)]
		\item Contrastive methods: choose contrastive points where the energy should be pushed up, and push down the energy at the data points.
		\item Architectural methods: build the machine so that the volume of the low energy regions is bounded, i.e., limiting the latent space. For example, in K-means, the latent space is discrete.
		\item Regularized latent variable methods: regularize the volume of the low energy regions, i.e., add regularization term on the latent variable to limit the information capacity of the latent representation.
	\end{enumerate}
\item $$l_{example}(x,y, \hat{y},W) = [F_W(x,y)-F_W(x,\hat{y}) +m ]^{+},$$
where $(x,\hat{y})$ are negative examples and $m>0$ is the margin. If $F_W(x,\hat{y}) - F_W(x,y) < m$, then to minimize $l_{example}(x,y, \hat{y},W)$, we need to push the difference of $F_W(x,\hat{y}) - F_W(x,y)$ becoming larger, which either push down $F_W(x,y)$ or push up $F_W(x,\hat{y})$.
\end{enumerate}

\subsection{Negative log-likelihood loss}
\begin{enumerate}[(i)]
	\item \begin{equation}\label{eq:likelihood}
	p_W(y|x) 
%	=\frac{\exp(-\beta F_W(x,y))}{\int_{y'}\exp(-\beta F_W(x,y'))}
	=\frac{\exp(-\beta F_W(x,y))}{\sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k))}, \; y\in\{1,\ldots,n\}, \; \beta>0.
	\end{equation}
	
	\item The negative log-likehood loss is from taking the logarithm of \eqref{eq:likelihood} and multiply with -1,
	\begin{equation}
	\begin{aligned}
	-\log p_W(y|x) & =- \log \frac{\exp(-\beta F_W(x,y))}{\sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k))} \\
	& = -[\log \exp(-\beta F_W(x,y)) - \log \sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k))]\\
	& = -(-\beta F_W(x,y)) -[- \log \sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k))]\\
	& = \beta F_W(x,y) + \log \sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k)) .
	\end{aligned}
	\end{equation}
	
	Multiply the loss with $\frac{1}{\beta}$, we obtain:
	\begin{equation}\label{eq:loss}
	l(x,y, W) = F_W(x,y) + \frac{1}{\beta}\log \sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k)) .
	\end{equation}
	
	\item The gradient of \eqref{eq:loss} wrt $W$ is:
	\begin{equation}\label{eq:dldW}
	\begin{aligned}
	\frac{\partial l(x,y, W)}{\partial W} & = \frac{\partial}{\partial W}[F_W(x,y) + \frac{1}{\beta}\log \sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k)) ]\\
	& = \frac{\partial F_W(x,y)}{\partial W} + \frac{1}{\beta}  \frac{\partial}{\partial W} \log \sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k)) \\
	& = \frac{\partial F_W(x,y)}{\partial W} + \frac{1}{\beta} \frac{\frac{\partial}{\partial W}\sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k))  }{\sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k))  }\\
	& = \frac{\partial F_W(x,y)}{\partial W} + \frac{1}{\beta} \frac{ \sum\limits_{k=1}^{n} \frac{\partial \exp(-\beta F_W(x,k))}{\partial W} }{\sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k))  }\\
	& = \frac{\partial F_W(x,y)}{\partial W} + \frac{1}{\beta} \frac{ \sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k)) (-\beta \frac{\partial F_W(x,k)}{\partial W}) }{\sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k))  }\\
	& = \frac{\partial F_W(x,y)}{\partial W} - \sum\limits_{y'=1}^{n} \frac{\exp(-\beta F_W(x,y'))}{\sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k))  } \frac{\partial F_W(x,y')}{\partial W}\\
	& = \frac{\partial F_W(x,y)}{\partial W} -  \sum\limits_{y'=1}^{n}  p_W(y'|x) \frac{\partial F_W(x,y')}{\partial W},
	\end{aligned}
	\end{equation}
	where the last quality use \eqref{eq:likelihood}.
	
	The gradient in \eqref{eq:dldW} requires compute likelihood $p_W(y'|x)$ and $\frac{\partial F_W(x,y')}{\partial W}$ for all $y'\in {1,\ldots,n}$ which includes computation of $\exp(-\beta F_W(x,y')) $  for all $y'\in {1,\ldots,n}$. When $n$ is large, this computation is very costly, thus intractable. To overcome it, we can use sampling method (like MCMC, which does not require knowing the normalization constant $\sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k)) $) to draw a sample $\hat{y}$ from $p_W(y|x)$ and use it to evaluate the second term in the last equality of \eqref{eq:dldW}, i.e, 
	\begin{equation}
\frac{\partial l(x,y, W)}{\partial W} = \frac{\partial F_W(x,y)}{\partial W} - \frac{\partial F_W(x,\hat{y})}{\partial W}, \; \text{where } \hat{y}\text{ is a sample from } p_W(y|x).
	\end{equation}
	
	\item From \eqref{eq:loss}, we know to minimize the loss $l$, we need to make the part $F_W(x,y)$ small as well as the part $ \frac{1}{\beta}\log \sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k)) $. To make the part $ \frac{1}{\beta}\log \sum\limits_{k=1}^{n}\exp(-\beta F_W(x,k)) $ small, we need to make $F_W(x,k)$ as large as possible. To achieve this, the loss is minimized when $F_W(x,y)$ to $-\infty$ and $F_W(x,k)$ to $\infty$. This can be also seen in the gradient descent, assume the step length $\eta>0$, the update of parameter $W$ is 
	\begin{equation}
	W \leftarrow W - \eta \frac{\partial F_W(x,y)}{\partial W} + \eta  \sum\limits_{y'=1}^{n}  p_W(y'|x) \frac{\partial F_W(x,y')}{\partial W},
	\end{equation}
	the part $- \eta \frac{\partial F_W(x,y)}{\partial W}$ will push down the energy at samples $(x,y)$, while $F_W(x, k)$ is low energy, the weights $p_W(y'|x)$ is larger, so the term $+ \eta  \sum\limits_{y'=1}^{n}  p_W(y'|x) \frac{\partial F_W(x,y')}{\partial W}$ will pull up the energy at these points $(x,y')$, and will continue doing so as long as $\frac{\partial F_W(x,y)}{\partial W} \neq 0$ and $\frac{F_W(x,y')}{\partial W} \neq 0$, i.e., $F_W(x,y)$ do not decrease, i.e., $-\infty$ and $F_W(x,y')$ do not increase, i.e., $\infty$, as long as $y'\neq y$. So this will resulting an energy space with sharp edges in continuous region, since every $y'\neq y$ will have positive infinite energy but at $y$ is negative infinite.
	
\end{enumerate}


\subsection{Comparing contrastive loss functions}
\begin{enumerate}[(a)]
	\item 
	\begin{equation}
	\frac{\partial l_\text{simple} (x,y, \bar{y}, W)   }{\partial W}=\left\lbrace 
	\begin{aligned}
	&\frac{\partial F_W(x, y)}{\partial W} -\frac{\partial F_W(x, \bar{y})}{\partial W},  && \text{ if } F_W(x,y) \geq 0 \text{ and } F_W(x, \bar{y}) \leq m,\\
	&	\frac{\partial F_W(x, y)}{\partial W},  && \text{ if } F_W(x,y) \geq 0 \text{ and } F_W(x, \bar{y}) > m,\\
	&	-\frac{\partial F_W(x, \bar{y})}{\partial W},  && \text{ if } F_W(x,y) < 0 \text{ and } F_W(x, \bar{y}) \leq m,\\
	&	0,  && \text{ if } F_W(x,y) < 0 \text{ and } F_W(x, \bar{y}) > m.\\
	\end{aligned}\right. 
	\end{equation}
	
	\item 
	\begin{equation}
	\frac{\partial l_\text{hinge} (x,y, \bar{y}, W)   }{\partial W}=\left\lbrace 
	\begin{aligned}
& \frac{\partial F_W(x, y)}{\partial W} -\frac{\partial F_W(x, \bar{y})}{\partial W}, &&  \text{ if } F_W(x, \bar{y}) -F_W(x, y) \leq m, \\
& 0, && \text{ if } F_W(x, \bar{y}) -F_W(x, y) > m.
	\end{aligned}
	\right. 
	\end{equation}
	
	\item 
	\begin{equation}
	\frac{\partial l_\text{square-square} (x,y, \bar{y}, W)   }{\partial W}=
	2[F_W(x,y)]^+ \frac{\partial F_W(x, y)}{\partial W} - 2[m-F_W(x,\bar{y})]^+ \frac{\partial F_W(x, \bar{y})}{\partial W}.
	\end{equation}
	
	\item 
	\begin{enumerate}[(i)]
		\item The NLL loss result in sharp edges in energy space, push down the energy at data points $(x,y)$ until reaching negative infinity and push up the energy at contrastive points $(x, \bar{y})$ until reaching the positive infinity. While the three losses above only want to make the energy difference between $(x,y)$ and $(x, \bar{y})$ to be larger than the margin $m$ (hinge loss) or separated by 0 and margin $m$ (simple and square-square loss). When the energy $F_W(x,y)$ and $F_W(x, \bar{y})$ is separated by the margin $m$, i.e., $F_W(x,\bar{y})-F_W(x,y) >m$ for hinge loss, $F_W(x,\bar{y})>m$ and $F_W(x,y) <0$ for simple and square-square loss, the model stops pushing $F_W(x,\bar{y})$ up or $F_W(x,{y})$ down.
		
		\item The margin $m$ in hinge loss is the distance we want to make between the energy $F_W(x,\bar{y})$ and $F_W(x,
		{y})$, minimizing the hinge loss makes $F_W(x,y)$ lower that $F_W(x,\bar{y})$ by margin $m$. The reason we only take positive part of $F_W(x,y) -F_W(x, \bar{y})+m$ is when  $F_W(x,y) -F_W(x, \bar{y})+m$ is negative, the difference $F_W(x,\bar{y})-F_W(x,y)$ is already larger than $m$, so we don't need to push the energy thus the gradient $\frac{\partial l_\text{hinge} (x,y, \bar{y}, W)   }{\partial W}= 0$. But if $F_W(x,\bar{y})-F_W(x,y) < m$, we need to push up $F_W(x,\bar{y})$ and push down $F_W(x,y)$, and the decent direction (negative gradient)$-\frac{\partial l_\text{hinge} (x,y, \bar{y}, W)   }{\partial W}= -\frac{\partial F_W(x, y)}{\partial W} +\frac{\partial F_W(x, \bar{y})}{\partial W}$ shows by updating in this direction, we can 
		push up $F_W(x,\bar{y})$ and push down $F_W(x,y)$ in order to make their difference larger than $m$.
		
		\item The simple loss and square-square loss consider the energy values of $F_W(x,\bar{y})$ and $F_W(x,y)$, and separate them by making the $F_W(x,y) <0$ and $F_W(x,\bar{y})>m$, while hinge loss only cares about the difference, i.e., making $F_W(x,\bar{y})-F_W(x,y) >m$.
		
		When I want a positive/nonnegative energy function, I would choose the square-square loss, since when $F_W(x,y)>0$ but very close to 0, the model update to push the energy down is very small ($2[F_W(x,y)]^+ \frac{\partial F_W(x, y)}{\partial W} $ depends on the value of $F_W(x,y)$, smaller positive $F_W(x,y)$ brings smaller update).
		When I don't require positive energy function, I can use simple loss. The other difference is that square-square loss is smoother, it is differentiable wrt $F_W(x,y)$ and  $F_W(x,\bar{y})$.
	\end{enumerate}
	
	
\end{enumerate}

\end{document}